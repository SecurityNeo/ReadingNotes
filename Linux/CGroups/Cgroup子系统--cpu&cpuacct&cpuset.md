# CGroup子系统--cpu&cpuacct&cpuset #

摘自[Cgroup分析之cpu、cpuacct](https://blog.csdn.net/tanzhe2017/article/details/81001105)

## cpu子系统 ##

Cpu子系统可以通过一些用户态接口文件来实现对cpu资源访问的控制，每个文件都独立存在在cgroup虚拟文件系统的伪文件中，如下：


- cpu.cfs_period_us

	以微秒为单位指定一个period的长度。

- cpu.cfs_quota_us

	以微秒为单位指定一个period中可用运行时间。

- cpu.stat

	当前nr_periods、nr_throttled、throttled_time的状态值，其中：
	
	nr_periods：已执行完的period数。
	
	nr_throttled：当前cgroup中所有进程超过限制值的次数。
	
	throttled_time：当前cgroup中所有进程超过限制值的持续时间。

- cpu.shares

	包含用来指定在cgroup中的任务可用的相对共享cpu时间的整数值。
	例如：在两个cgroup中都将cpu.shares设定为1的任务将有相同的cpu时间，但在cgroup中将cpu.shares设定为2的任务可使用的cpu时间是在cgroup中将cpu.shares设定为1的任务可使用的cpu时间的两倍。

- cpu.rt_period_us

	以微秒（μs，这里以"us"代表）为单位指定在某个时间段中cgroup对cpu资源访问重新分配的频率。如果某个cgroup中的任务应该每5秒钟有4秒时间可访问cpu资源，则将cpu.rt_runtime_us设定为4000000，并将cpu.rt_period_us设定为5000000。

- cpu.rt_runtime_us

	以微秒（μs，这里以"us"代表）为单位指定在某个时间段中cgroup中的任务对cpu资源的最长连续访问时间。建立这个限制是为了防止一个cgroup中的任务独占cpu时间。


## cpuacct子系统 ##

cpuacct主要是根据内核现有的一些接口对cpu使用状况做统计，并自动生成cgroup中进程所使用的CPU资源报告。相关用户态接口如下：

- cpuacct.stat

	报告当前cgroup和子组的所有任务使用用户模式和系统模式消耗的CPU周期数（单位由系统中user_hz定义）。

- cpuacct.usage

	统计这个cgroup中所有任务消耗的总cpu时间（纳秒）。

- cpuacct.usage_percpu

	统计这个cgroup中所有任务在每一个cpu上分别消耗的cpu时间（纳秒）。


## cpuset子系统 ##

cpuset主要用于设置CPU的亲和性，可以限制cgroup中的进程只能在指定的CPU上运行，或者不能在指定的CPU上运行，同时cpuset还能设置内存的亲和性。

**NUMA**

[维基](https://en.wikipedia.org/wiki/Non-uniform_memory_access)

NUMA,非一致存储访问结构(Non-uniform Memory Access),介于SMP(多处理器结构)和MPP(海量并行处理结构)之间。NUMA 服务器的基本特征是具有多个CPU模块，每个CPU模块由多个CPU组成，并且具有独立的本地内存、I/O槽口等。其节点之间可以通过互联模块进行连接和信息交互，故而每颗CPU可以访问整个系统的内存。但是这种结构有一个非常重要的缺陷：访问远地内存的延时远远超过本地内存，当CPU数量增加时，系统的性能并不会线性增加。

在系统中执行`numactl --harware`命令可以查看本机的NUMA架构信息：

```
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
node 0 size: 127834 MB
node 0 free: 72415 MB
node 1 cpus: 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
node 1 size: 128990 MB
node 1 free: 68208 MB
node distances:
node   0   1
  0:  10  21
  1:  21  10
```

右上可知系统有两个节点，每个节点有16颗CPU，一个NUMA节点包括一个内存节点和属于同一块CPU的若干个逻辑核,我们关心`node distances`部分，CPU访问本节点的内存的通信成本是常量值10，操作系统以此基准来量化访问其他NUMA节点上内存的代价。

**用户态接口**

[https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpuset](https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpuset)

- cpuset.cpus（强制）

	指​​​定​​​允​​​许​​​这​​​个​​​cgroup中​​​任​​​务​​​访​​​问​​​的​​​CPU。​​​如配置`0-2,16`表示使用CPU 0、​​​1、​​​2和​​​16。

- cpuset.mems（强​​​制​​​）

	指​​​定​​​允​​​许​​​这​​​个​​​cgroup中​​​任​​​务​​​可​​​访​​​问​​​的​​​内​​​存​​​节​​​点​​​。​​​如配置`0-2,16`表示使用​​​内​​​存​​​节​​​点​​​ 0、​​​1、​​​2和​​​16。

- cpuset.memory_migrate

	​​​指​​​定​​​当​​​cpuset.mems中​​​的​​​值​​​更​​​改​​​时​​​是​​​否​​​应​​​该​​​将​​​内​​​存​​​中​​​的​​​页​​​迁​​​移​​​到​​​新​​​节​​​点（0或​​​者​​​1）。​​​默​​​认​​​情​​​况​​​下​​​禁​​​止​​​内​​​存​​​迁​​​移​​​（0）且​​​页​​​就​​​保​​​留​​​在​​​原​​​来​​​分​​​配​​​的​​​节​​​点​​​中​​​，即​​​使​​​在​​​cpuset.mems中​​​现​​​已​​​不​​​再​​​指​​​定​​​这​​​个​​​节​​​点​​​。​​​如​​​果​​​启​​​用​​​（1），则​​​该​​​系​​​统​​​会​​​将​​​页​​​迁​​​移​​​到​​​由​​​cpuset.mems指​​​定​​​的​​​新​​​参​​​数​​​中​​​的​​​内​​​存​​​节​​​点​​​中​​​，可​​​能​​​的​​​情​​​况​​​下​​​保​​​留​​​其​​​相​​​对​​​位​​​置，例​​​如​​​：原​​​来​​​由​​​cpuset.mems指​​​定​​​的​​​列​​​表​​​中​​​第​​​二​​​个​​​节​​​点​​​中​​​的​​​页​​​将​​​会​​​重​​​新​​​分​​​配​​​给​​​现​​​在​​​由​​​cpuset.mems指​​​定​​​的​​​列​​​表​​​的​​​第​​​二​​​个​​​节​​​点​​​中​​​，如​​​果​​​这​​​个​​​位​​​置​​​是​​​可​​​用​​​的​​​。

- cpuset.cpu_exclusive

	​​​指​​​定​​​是​​​否​​​其​​​它​​​cpuset及​​​其​​​上​​​、​​​下​​​级​​​族​​​群​​​可​​​共​​​享​​​为​​​这​​​个​​​cpuset指​​​定​​​的​​​CPU（0或​​​者​​​1）。​​​默​​​认​​​情​​​况​​​下​​​（0）CPU不​​​是​​​专​​​门​​​分​​​配​​​给​​​某​​​个​​​cpuset的​​​。

- cpuset.mem_exclusive

	指​​​定​​​是​​​否​​​其​​​它​​​cpuset可​​​共​​​享​​​为​​​这​​​个​​​cpuset指​​​定​​​的​​​内​​​存​​​节​​​点（0 或​​​者​​​ 1）。​​​默​​​认​​​情​​​况​​​下​​​（0）内​​​存​​​节​​​点​​​不​​​是​​​专​​​门​​​分​​​配​​​给​​​某​​​个​​​cpuset的​​​。​​​专​​​门​​​为​​​某​​​个​​​cpuset保​​​留​​​内​​​存​​​节​​​点​​​（1）与​​​使​​​用​`cpuset.mem_hardwall`启​​​用​​​内​​​存​​​hardwall功​​​能​​​是​​​一​​​致​​​的​​​。

- cpuset.mem_hardwall​​​​​​​​​

	​​​指​​​定​​​是​​​否​​​应​​​将​​​内​​​存​​​页​​​面​​​的​​​内​​​核​​​分​​​配​​​限​​​制​​​在​​​为​​​这​​​个​​​cpuset指​​​定​​​的​​​内​​​存​​​节​​​点​​​（0或​​​者​​​1）。​​​默​​​认​​​情​​​况​​​下​​​为​​​0，属​​​于​​​多​​​个​​​用​​​户​​​的​​​进​​​程​​​共​​​享​​​页​​​面​​​和​​​缓​​​冲​​​。​​​启​​​用​​​hardwall时​​​（1）每​​​个​​​任​​​务​​​的​​​用​​​户​​​分​​​配​​​应​​​保​​​持​​​独​​​立​​​。

- cpuset.memory_pressure​​​

	包​​​含​​​运​​​行​​​在​​​这​​​个​​​cpuset中​​​产​​​生​​​的​​​平​​​均​​​内​​​存​​​压​​​力​​​的​​​只​​​读​​​文​​​件​​​。​​​启​​​用​​​ `cpuset.memory_pressure_enabled`时​​​，这​​​个​​​伪​​​文​​​件​​​中​​​的​​​值​​​会​​​自​​​动​​​更​​​新​​​，否​​​则​​​伪​​​文​​​件​​​包​​​含​​​的​​​值​​​为​​​0。

- cpuset.memory_pressure_enabled

	指​​​定​​​系​​​统​​​是​​​否​​​应​​​该​​​计​​​算​​​这​​​个​​​cgroup中​​​进​​​程​​​所​​​生​​​成​​​内​​​存​​​压​​​力​​​​​​（0或​​​者​​​1）。​​​计​​​算​​​出​​​的​​​值​​​会​​​输​​​出​​​到​​​`cpuset.memory_pressure`，且​​​代​​​表​​​进​​​程​​​试​​​图​​​释​​​放​​​使​​​用​​​中​​​内​​​存​​​的​​​比​​​例​​​，报​​​告​​​为​​​尝​​​试​​​每​​​秒​​​再​​​生​​​内​​​存​​​的​​​整​​​数​​​值​​​再​​​乘​​​1000。
​​​
- cpuset.memory_spread_page

	​​​指​​​定​​​是​​​否​​​应​​​将​​​文​​​件​​​系​​​统​​​缓​​​冲​​​平​​​均​​​分​​​配​​​给​​​这​​​个​​​cpuset的​​​内​​​存​​​节​​​点​​​（0或​​​者​​​1）。​​​默​​​认​​​情​​​况​​​为​​​ 0，不​​​尝​​​试​​​为​​​这​​​些​​​缓​​​冲​​​平​​​均​​​分​​​配​​​内​​​存​​​页​​​面​​​，且​​​将​​​缓​​​冲​​​放​​​置​​​在​​​运​​​行​​​生​​​成​​​缓​​​冲​​​的​​​进​​​程​​​的​​​同​​​一​​​节​​​点​​​中​​​。​​​

- cpuset.memory_spread_slab

	​​​指​​​定​​​是​​​否​​​应​​​在​​​cpuset间​​​平​​​均​​​分​​​配​​​用​​​于​​​文​​​件​​​输​​​入​​​/输​​​出​​​操​​​作​​​的​​​内​​​核​​​缓​​​存​​​板​​​（0或​​​者​​​1）。​​​默​​​认​​​情​​​况​​​是​​​0，即​​​不​​​尝​​​试​​​平​​​均​​​分​​​配​​​内​​​核​​​缓​​​存​​​板​​​，并​​​将​​​缓​​​存​​​板​​​放​​​在​​​生​​​成​​​这​​​些​​​缓​​​存​​​的​​​进​​​程​​​所​​​运​​​行​​​的​​​同​​​一​​​节​​​点​​​中​​​。​​​

- cpuset.sched_load_balance

	指​​​定​​​是​​​否​​​在​​​这​​​个​​​cpuset中​​​跨​​​CPU平​​​衡​​​负​​​载​​​内​​​核​​​（0或​​​者​​​1）。​​​默​​​认​​​情​​​况​​​是​​​1，即​​​内​​​核​​​将​​​超​​​载​​​CPU 中​​​的​​​进​​​程​​​移​​​动​​​到​​​负​​​载​​​较​​​低​​​的​​​CPU中​​​以​​​便​​​平​​​衡​​​负​​​载​​​。​​​

- cpuset.sched_relax_domain_level
	
	包​​​含​​​-1到​​​小​​​正​​​数​​​间​​​的​​​整​​​数​​​，它​​​代​​​表​​​内​​​核​​​应​​​尝​​​试​​​平​​​衡​​​负​​​载​​​的​​​CPU宽​​​度​​​范​​​围​​​。​​​如​​​果​​​禁​​​用​​​了​​​ `cpuset.sched_load_balance`，则​​​该​​​值​​​毫​​​无​​​意​​​义​​​。​​​
